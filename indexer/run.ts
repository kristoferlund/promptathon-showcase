import 'dotenv/config';
import { join, dirname } from 'path';
import { fileURLToPath } from 'url';
import { existsSync, mkdirSync, writeFileSync } from 'fs';
import {
  PageIndexer,
  loadSubmissions,
  serializeSubmissions,
} from './src/index.js';
import type { Submission } from './src/csv.js';
import type { PageSnapshot } from './src/types.js';
import type { SubmissionInput } from './src/indexer.js';
import crypto from 'crypto';

const __dirname = dirname(fileURLToPath(import.meta.url));

// Limit number of submissions to process (useful during development)
const limit = process.env.LIMIT ? parseInt(process.env.LIMIT, 10) : undefined;

// Output paths
const csvPath = join(__dirname, 'submissions.csv');
const sqlOutputPath = join(__dirname, '..', 'src', 'server', 'src', 'seeds', 'seed_apps.sql');
const imagesDir = join(__dirname, 'images');

// Ensure images directory exists
mkdirSync(imagesDir, { recursive: true });

// Load all submissions from CSV (used for write-back)
const allSubmissions = loadSubmissions(csvPath);

// Working set may be limited during development
let submissions = allSubmissions;
if (limit && limit > 0) {
  submissions = allSubmissions.slice(0, limit);
  console.log(`Limiting to first ${limit} submissions (set LIMIT env var to change)\n`);
}

// Determine which submissions need processing
function imageId(url: string): string {
  return crypto.createHash('md5').update(url).digest('hex');
}

function hasImages(sub: Submission): boolean {
  const id = imageId(sub.appUrl);
  return (
    existsSync(join(imagesDir, `${id}_1500.jpg`)) &&
    existsSync(join(imagesDir, `${id}_300.jpg`))
  );
}

function hasAiData(sub: Submission): boolean {
  return sub.aiTitle.length > 0 && sub.aiDescription.length > 0;
}

function needsProcessing(sub: Submission): boolean {
  return !hasImages(sub) || !hasAiData(sub);
}

const toProcess = submissions.filter(needsProcessing);
const alreadyDone = submissions.filter((s) => !needsProcessing(s));

console.log(`Starting indexer...`);
console.log(`  Total submissions: ${submissions.length}`);
console.log(`  Already complete: ${alreadyDone.length}`);
console.log(`  Need processing: ${toProcess.length}`);
console.log(`  SQL output: ${sqlOutputPath}`);
console.log(`  Images output: ${imagesDir}\n`);

// Build a lookup map for updating submissions in-place (covers all rows)
const submissionsByUrl = new Map<string, Submission>();
for (const s of allSubmissions) {
  submissionsByUrl.set(s.appUrl, s);
}

// SQL generation helpers
function escapeSql(str: string): string {
  return str.replace(/'/g, "''");
}

function sqlOpt(val: string | null | undefined): string {
  if (!val) return 'NULL';
  return `'${escapeSql(val)}'`;
}

function extractCanisterId(url: string): string | null {
  const match = url.match(/https?:\/\/([a-z0-9-]+)\.(icp0\.io|ic0\.app|raw\.ic0\.app)/);
  if (match && match[1] && match[1].includes('-')) {
    return match[1];
  }
  return null;
}

function generateSql(sub: Submission): string {
  const id = imageId(sub.appUrl);
  const canisterId = extractCanisterId(sub.appUrl);
  return `INSERT INTO app (url, canister_id, title, description, image_id, author_name, app_name, social_post_url) VALUES (${sqlOpt(sub.appUrl)}, ${sqlOpt(canisterId)}, ${sqlOpt(sub.aiTitle)}, ${sqlOpt(sub.aiDescription)}, ${sqlOpt(id)}, ${sqlOpt(sub.authorName)}, ${sqlOpt(sub.appName)}, ${sqlOpt(sub.socialPostUrl)});`;
}

// Process submissions that need it
async function run() {
  if (toProcess.length > 0) {
    const indexer = new PageIndexer({
      openaiApiKey: process.env.OPENAI_API_KEY,
      anthropicApiKey: process.env.ANTHROPIC_API_KEY,
      concurrency: 3,
      timeout: 30000,
    });

    const inputs: SubmissionInput[] = toProcess.map((s) => ({
      url: s.appUrl,
      meta: {
        authorName: s.authorName,
        appName: s.appName,
        socialPostUrl: s.socialPostUrl,
      },
    }));

    await indexer.processSubmissions(inputs, async (snapshot: PageSnapshot) => {
      if (snapshot.status === 'ok') {
        const id = imageId(snapshot.url);

        // Save screenshots
        writeFileSync(join(imagesDir, `${id}_1500.jpg`), snapshot.screenshotLarge);
        writeFileSync(join(imagesDir, `${id}_300.jpg`), snapshot.screenshotSmall);

        // Update the submission with AI data and persist immediately
        const sub = submissionsByUrl.get(snapshot.url);
        if (sub) {
          sub.aiTitle = snapshot.aiTitle;
          sub.aiDescription = snapshot.aiDescription;
        }
        writeFileSync(csvPath, serializeSubmissions(allSubmissions), 'utf-8');

        console.log(`[ok] ${snapshot.submission.appName} by ${snapshot.submission.authorName}`);
        console.log(`     ${snapshot.aiTitle}`);
      } else {
        console.error(`[error] ${snapshot.url}: ${snapshot.error}`);
      }
    });
  }

  // Generate SQL seed from all submissions that have complete data
  const sqlStatements: string[] = [];
  sqlStatements.push('-- Generated by the indexer. Do not edit manually.');
  sqlStatements.push(`-- Generated at: ${new Date().toISOString()}`);
  sqlStatements.push(`-- Submissions processed: ${allSubmissions.length}`);
  sqlStatements.push('');

  let count = 0;
  for (const sub of allSubmissions) {
    if (hasAiData(sub) && hasImages(sub)) {
      sqlStatements.push(generateSql(sub));
      count++;
    }
  }

  writeFileSync(sqlOutputPath, sqlStatements.join('\n') + '\n', 'utf-8');
  console.log(`Seed file written: ${sqlOutputPath}`);
  console.log(`  ${count} INSERT statements`);
}

run().catch((err) => console.error('\nFatal error:', err));
